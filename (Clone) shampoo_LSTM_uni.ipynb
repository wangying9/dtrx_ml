{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eb5e415-48db-485b-8ac2-8e4449af6959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4be57cc-d546-4c0d-afbc-46a734eb91ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c825b0e-0276-4f2e-bed3-a3c01d0fb69f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0a337d9-5674-4150-8abf-33bde5bf1ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e4f83e4-df10-4897-a358-d7ccff792d29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('use catalog dbacademy')\n",
    "spark.sql('use schema labuser9128531_1738705451')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cef4d92-57b5-473d-97d0-3fe2ce0f26f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path='/Volumes/dbacademy/labuser9128531_1738705451/air/shampoo.csv'\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b093fcaf-9485-43da-80f0-d65bc0aa1a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# load and plot dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe0b144d-83c9-4759-9561-e6616d51920a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.toPandas()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "027bbe00-db18-414e-9816-bdacffaf11e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d78c848d-134b-4764-bf01-cddc3c87921c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "date=[]\n",
    "for i in df['Month']:\n",
    "    str_d='190'+i[0]+i[1:]\n",
    "    date.append(str_d)\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb468e1a-1952-48de-8e9f-9de5209edc77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "date_df = pd.DataFrame(date)\n",
    "df = pd.concat([df, date_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4630c96-7a45-4e3a-aed5-0ab0eda892f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81465b00-4826-4868-a7ee-dec20cab204b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21230cb1-55f4-4ab6-8fef-89dc6ab8641d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.rename(columns={0: 'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e126e7-a479-4c9c-a927-325565467047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25eedb4e-5355-4c9e-a69d-69ee45c90300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['Date']=pd.to_datetime(df['Date'], format='%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77eeadc4-f8cf-4297-83f7-e6d5d597d643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aee3113-d391-4692-a2f4-7f63b4686901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.drop('Month', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca366a1f-3954-4db2-8799-3880f48ecc8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581ce74b-812d-43e4-90b4-d9db6ff7c7ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a5a8723-033f-4969-9f71-c41cf54ea395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(df['Date'], df['Sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b57558e-a52a-46ed-b47e-19fa644a05b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Breaks down steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fb87c03-32c5-4e36-ad53-f37a17f4ea5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train and test setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64614c1b-9c36-45ae-9336-59681a8152e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# split data into train and test\n",
    "series=df['Sales']\n",
    "X = series.values\n",
    "train, test = X[0:-12], X[-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af82baa-496c-4e6b-be60-77cdf8973409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ef9c30e-a545-4bfe-be3d-add418c3f54b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Walk-forward model validation: current month will be used to predict the next month\n",
    "A rolling forecast scenario will be used, also called walk-forward model validation.\n",
    "Each time step of the test dataset will be walked one at a time. A model will be used to make a forecast for the time step, then the actual expected value from the test set will be taken and made available to the model for the forecast on the next time step.\n",
    "This mimics a real-world scenario where new Shampoo Sales observations would be available each month and used in the forecasting of the following month.\n",
    "\n",
    "Finally, all forecasts on the test dataset will be collected and an error score calculated to summarize the skill of the model. The root mean squared error (RMSE) will be used as it punishes large errors and results in a score that is in the same units as the forecast data, namely monthly shampoo sales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7641ea2b-307f-4c61-b297-df6bba6825fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# walk-forward validation\n",
    "history = [x for x in train] #only training data\n",
    "print(history)\n",
    "predictions = list()\n",
    "for i in range(len(test)):\n",
    "\t# make prediction\n",
    "\tpredictions.append(history[-1])\n",
    "\t# observation train+test\n",
    "\thistory.append(test[i])\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6827d90-cf25-4a81-a20c-0eaa8748e10b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions #start from the last element of the training data and go to the test data, the last one is the second last of the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf072d99-b596-4410-b420-a2591435d738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Preformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0aaa262-30a6-4bd4-907f-67e7d62dbb16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c493b47-f830-4eed-a1fb-bffc7c1d81b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test[:5], predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc15674-4109-4c63-bee1-46bdb6e55743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# report performance\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('RMSE: %.3f' % rmse)\n",
    "# line plot of observed vs predicted\n",
    "pyplot.plot(test)\n",
    "pyplot.plot(predictions)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "737b123b-0466-4ab1-b437-7649a82f920a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Establish baseline performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57e4acad-b3b2-4775-8978-efbebde98143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "https://machinelearningmastery.com/persistence-time-series-forecasting-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a529534-ce51-4250-be51-87a8a4599a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a24728d-3f27-4e1d-af3f-b768e62b145e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test all functions for data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5bff851-0ec3-47ac-a363-ac0d0d10ec76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Transform to a supervised learning problem\n",
    "The LSTM model in Keras assumes that your data is divided into input (X) and output (y) components.\n",
    "For a time series problem, we can achieve this by using the observation from the last time step (t-1) as the input and the observation at the current time step (t) as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c78d11d3-ac8c-46e4-b31c-c8cbc9072edc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1): #lag is 1 meaning previous value predict current value\n",
    "\tdf = DataFrame(data)\n",
    "    #push all values in a series down by a specified number places\n",
    "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
    "\tcolumns.append(df)\n",
    "    # concatenate these two series together to create a DataFrame ready for supervised learning\n",
    "\tdf = concat(columns, axis=1)\n",
    "\tdf.fillna(0, inplace=True)\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf17bc8-e7e9-4ed9-8ec3-aed65b4fd746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c180f6e-15b6-4f1e-8056-ede603adedd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# transform to supervised learning\n",
    "X = df.Sales.values\n",
    "supervised = timeseries_to_supervised(X, 1)\n",
    "print(supervised.head(),type(supervised))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ceed7b08-e73b-43fe-b681-ab34816d33e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Transform to Stationary\n",
    "This means that there is a structure in the data that is dependent on the time. Specifically, there is an increasing trend in the data.\n",
    "\n",
    "Stationary data is easier to model and will very likely result in more skillful forecasts.\n",
    "The trend can be removed from the observations, then added back to forecasts later to return the prediction to the original scale and calculate a comparable error score\n",
    "A standard way to remove a trend is by differencing the data. That is the observation from the previous time step (t-1) is subtracted from the current observation (t). This removes the trend and we are left with a difference series, or the changes to the observations from one time step to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8930fa5f-e98d-45e8-8701-2c748d353b73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be220f4-6a33-442a-af85-042fe21a7886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "596009e5-405f-4e85-9719-40882755dd6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.Sales[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "449f2b70-b32f-43a9-bee5-23741d5a487f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset.iloc[i] - dataset.iloc[i - interval]\n",
    "        # print(value)\n",
    "        diff.append(value)\n",
    "    print(diff)\n",
    "    return Series(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24f11cad-bdf1-4fc2-b83a-1dba5a44d168",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transform to stationay"
    }
   },
   "outputs": [],
   "source": [
    "# transform to be stationary\n",
    "differenced = difference(df.Sales, 1)\n",
    "print(differenced.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6a1d57-2fcb-4df3-9e37-fbdfb26a1425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "differenced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d8d3b5-829a-474f-942c-7bb631c2b6ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(df),len(differenced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9e7104-2a36-4e23-9e48-3381ad7c7fb8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inverse function"
    }
   },
   "outputs": [],
   "source": [
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66bf4043-32e8-4670-b3d6-33ca732a07f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# invert transform\n",
    "inverted = list()\n",
    "for i in range(len(differenced)):#35\n",
    "\tvalue = inverse_difference(df.Sales, differenced[i], len(df.Sales)-i)\n",
    "\tinverted.append(value)\n",
    "inverted = Series(inverted)\n",
    "print(inverted.head(), len(inverted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71a809c1-2e21-485b-9642-130b25964166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Transform to Scale\n",
    "Like other neural networks, LSTMs expect data to be within the scale of the activation function used by the network.\n",
    "\n",
    "The default activation function for LSTMs is the hyperbolic tangent (tanh), which outputs values between -1 and 1. This is the preferred range for the time series data.\n",
    "\n",
    "To make the experiment fair, the scaling coefficients (min and max) values must be calculated on the training dataset and applied to scale the test dataset and any forecasts. This is to avoid contaminating the experiment with knowledge from the test dataset, which might give the model a small edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5103635-1523-4217-b308-49984a2fc5e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "MinMaxScaler transform the data [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb6e4e4b-beab-454a-90a9-80a451217322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b57e6c56-68f8-48a3-90df-fc8b65f54491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# transform scale\n",
    "X = df.values\n",
    "X = X.reshape(len(X), 1)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = scaler.fit(X)\n",
    "scaled_X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eca9b31-f154-47bb-b1de-4943597d5a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scaled_X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb82de78-1899-4005-8194-5e15892ee5cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# invert transform\n",
    "inverted_X = scaler.inverse_transform(scaled_X)\n",
    "print(inverted_X[:5])\n",
    "inverted_series = Series(inverted_X[:, 0])\n",
    "print(inverted_series.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61920f26-5abb-4493-9c86-164b291425a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### LSTM development\n",
    "The Long Short-Term Memory network (LSTM) is a type of Recurrent Neural Network (RNN).\n",
    "\n",
    "A benefit of this type of network is that it can learn and remember over long sequences and does not rely on a pre-specified window lagged observation as input.\n",
    "In Keras, this is referred to as stateful, and **involves setting the “stateful” argument to “True” when defining an LSTM layer**.\n",
    "\n",
    "By default, an LSTM layer in Keras maintains state between data within one batch. A batch of data is a fixed-sized number of rows from the training dataset that defines how many patterns to process before updating the weights of the network. **State in the LSTM layer between batches is cleared by default, therefore we must make the LSTM stateful.** This gives us fine-grained control over when state of the LSTM layer is cleared, by calling the reset_states() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f37936d4-6635-4976-b65d-67eb4a8396d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The LSTM layer expects input to be in a matrix with the dimensions: [samples, time steps, features].\n",
    "\n",
    "`Samples`: These are independent observations from the domain, typically rows of data.\n",
    "`Time steps`: These are separate time steps of a given variable for a given observation.\n",
    "`Features`: These are separate measures observed at the time of observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b493f5b1-f7c9-4881-a94f-c8c025748297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e99ddf92-64db-4e4b-901b-8c8e90421532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "379c0a8f-82f3-4f89-8d28-509a5bd91d53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_values=df.values\n",
    "raw_values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8297b3ab-8744-47ca-a463-98537ba25398",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "transform data to be stationary"
    }
   },
   "outputs": [],
   "source": [
    "diff_values = difference(df.Sales, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "315e6ed2-7cea-4f87-9451-9acc249f1d69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "diff_values[:5], type(diff_values), len(diff_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1803391e-fef7-4188-9831-4c092e8a978e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "transform data to be supervised learning"
    }
   },
   "outputs": [],
   "source": [
    "supervised = timeseries_to_supervised(diff_values, 1)\n",
    "supervised_values = supervised.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f296547-0d72-47a4-b1a4-a765ee1c2480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "supervised_values[:5], type(supervised_values), len(supervised_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed18b44-89e6-4899-9105-20b96f2533c3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split data"
    }
   },
   "outputs": [],
   "source": [
    "train, test = supervised_values[0:-12], supervised_values[-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edceecaa-a3c4-490a-9564-18bcc994e1f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8431b9a-c87a-4549-a4c5-9359a3cb8975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(train), type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "139aaa20-81f1-4cff-9604-d9a708be0b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "556c88e8-def7-4804-8356-32bd096c49bb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scaling"
    }
   },
   "outputs": [],
   "source": [
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "    # fit scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train)\n",
    "\n",
    "    # transform train\n",
    "    print(train.shape)\n",
    "    train = train.reshape(train.shape[0], train.shape[1])  # 23,2\n",
    "    print(train.shape)\n",
    "    train_scaled = scaler.transform(train)\n",
    "    print(train_scaled.shape)\n",
    "    \n",
    "    # transform test\n",
    "    test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_scaled = scaler.transform(test)\n",
    "    return scaler, train_scaled, test_scaled\n",
    "\n",
    "scaler, train_scaled, test_scaled = scale(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67244973-9566-4036-be09-f8aa6dbdad39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be7fe0c0-9029-40f9-b401-bce93f272ccf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Shape train data for LSTM"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure train is a 2D array of shape (samples, features+1)\n",
    "X_train, y_train = train_scaled[:, 0:-1], train_scaled[:, -1] # X: input features, y: target value\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# Reshaping X to (samples, timesteps, features)\n",
    "## 1 timestep per sample, with X.shape[1] features\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1]) #23,1,1\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b7f8e6-fa58-408c-af11-da55997392f0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Shape test for LSTM"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure train is a 2D array of shape (samples, features+1)\n",
    "X_test, y_test = test_scaled[:, 0:-1], test_scaled[:, -1] # X: input features, y: target value\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Reshaping X to (samples, timesteps, features)\n",
    "## 1 timestep per sample, with X.shape[1] features\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1]) #23,1,1\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cf72c46-ea0a-4cc4-97d5-e188950d2eeb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check the shape"
    }
   },
   "outputs": [],
   "source": [
    "# Verify the shape of training and testing data\n",
    "print(f\"X shape: {X_train.shape}\")  # Should be (23, 1, 1)\n",
    "print(f\"y shape: {y_train.shape}\")  # Should be (23,)\n",
    "print(f\"X_test shape: {X_test.shape}\")  # Should be (num_samples_test, 1, 1)\n",
    "print(f\"y_test shape: {y_test.shape}\")  # Should be (num_samples_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c48b20-eb38-4228-a3c5-9ccfe29ac62b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check if null data"
    }
   },
   "outputs": [],
   "source": [
    "assert not np.any(np.isnan(X_train)), \"X contains NaN values\"\n",
    "assert not np.any(np.isnan(y_train)), \"y contains NaN values\"\n",
    "assert not np.any(np.isnan(X_test)), \"X_test contains NaN values\"\n",
    "assert not np.any(np.isnan(y_test)), \"y_test contains NaN values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dd743e2-481c-4dc3-be40-5b61eaa7a10c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LSTM function"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "def fit_lstm(Xtrain, ytrain,batch_size, nb_epoch, neurons):\n",
    "     # define model\n",
    "    model = Sequential()\n",
    "    # Use Input layer to define the input shape\n",
    "    # batch_size=1\n",
    "    model.add(Input(batch_shape=(batch_size, Xtrain.shape[1], 1)))\n",
    "    # Add LSTM layer with stateful=True for time series\n",
    "    model.add(LSTM(units=neurons, return_sequences=True, stateful=True))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    # early stopping to avoid overfitting\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(Xtrain, ytrain, epochs=1, batch_size=batch_size,\n",
    "        #  validation_data=(Xtrain, ytrain),\n",
    "         verbose=1, callbacks=[early_stop], shuffle=False)\n",
    "        # Reset states of the LSTM layer (model.layers[0] is the LSTM layer)\n",
    "        model.layers[0].reset_states()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4b94dc97-75f5-4006-9784-e0bc94e6af0d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test LSTM function"
    }
   },
   "outputs": [],
   "source": [
    "model1=fit_lstm(X_train, y_train,1, 50, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f312c0eb-5f1e-488e-9ef8-ba7e0ed3a6c4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LSTM model Structure"
    }
   },
   "outputs": [],
   "source": [
    "# # fit an LSTM network to training data\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, Dense\n",
    "# from keras.layers import Input\n",
    "# from keras.callbacks import EarlyStopping\n",
    "#  # define model\n",
    "# model = Sequential()\n",
    "# # Use Input layer to define the input shape\n",
    "# batch_size=1\n",
    "# model.add(Input(batch_shape=(batch_size, X.shape[1], 1)))\n",
    "# # Add LSTM layer with stateful=True for time series\n",
    "# model.add(LSTM(units=32, return_sequences=True, stateful=True))\n",
    "# model.add(Dense(1))\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571c97af-17f9-4c99-8041-a6be150213ac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "early stopping to avoid overfitting"
    }
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6798f43-96e3-45ad-b350-51a04160416f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     history = model.fit(X_train, y_train, epochs=50, batch_size=1, validation_data=(X_test, y_test), verbose=1, callbacks=[early_stop], shuffle=False)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during training: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a34d48fb-81b8-4ec2-b546-f1d2e8d5d877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Forcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfec67e1-eb8e-424a-bc6c-8302aea73703",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function for 1-step forcasting"
    }
   },
   "outputs": [],
   "source": [
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbfe9434-1fe8-4102-929c-3f54d8db5171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "280f73e0-2177-49a5-bf9e-2c9ff0e31c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = numpy.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e7d5b3-14a1-4e5b-8879-5586240bf273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# walk-forward validation on the test data\n",
    "predictions = list()\n",
    "for i in range(len(test_scaled)):#(0,12)\n",
    "\t# make one-step forecast\n",
    "\tyhat = forecast_lstm(model, 1, X_test[i])\n",
    " \n",
    "\t# invert scaling\n",
    "\tyhat = invert_scale(scaler, X_test[i], yhat)\n",
    " \n",
    "\t# invert differencing\n",
    "\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    " \n",
    "\t# store forecast\n",
    "\tpredictions.append(yhat)\n",
    "\texpected = raw_values[len(train) + i + 1]\n",
    "\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df9c672a-2549-4209-8926-13e950919a6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca72e0fa-1c77-403e-aa7b-5b35a23fbdee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Performance"
    }
   },
   "outputs": [],
   "source": [
    "# report performance\n",
    "rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f48f78-15c0-4f70-929d-d8353b652426",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Plot"
    }
   },
   "outputs": [],
   "source": [
    "# line plot of observed vs predicted\n",
    "pyplot.plot(raw_values[-12:])\n",
    "pyplot.plot(predictions)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acdf1cc7-5d30-4a83-9928-097da8764881",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "repeat exp for 30 times"
    }
   },
   "outputs": [],
   "source": [
    "# repeat experiment\n",
    "repeats = 30\n",
    "error_scores = list()\n",
    "for r in range(repeats):\n",
    "    # fit the model\n",
    "    lstm_model = fit_lstm(X_train,y_train, 1, 3000, 4) # 3000 epoch, 4 neurons\n",
    "\n",
    "    # walk-forward validation on the test data\n",
    "    predictions = list()\n",
    "    for i in range(len(test_scaled)):#(0,12)\n",
    "        # make one-step forecast\n",
    "        yhat = forecast_lstm(lstm_model, 1, X_test[i])\n",
    "    \n",
    "        # invert scaling\n",
    "        yhat = invert_scale(scaler, X_test[i], yhat)\n",
    "    \n",
    "        # invert differencing\n",
    "        yhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "    \n",
    "        # store forecast\n",
    "        predictions.append(yhat)\n",
    "        rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
    "        print('%d) Test RMSE: %.3f' % (r+1, rmse))\n",
    "        error_scores.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd0ceac4-4acc-4a6e-91a6-b304dc99295f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "summarize results"
    }
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "results = DataFrame()\n",
    "results['rmse'] = error_scores\n",
    "print(results.describe())\n",
    "results.boxplot()\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) shampoo_LSTM_uni",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
